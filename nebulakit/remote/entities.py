"""
This module contains shadow entities for all Nebula entities as represented in Nebula Admin / Control Plane.
The goal is to enable easy access, manipulation of these entities.
"""
from __future__ import annotations

from typing import Dict, List, Optional, Tuple, Union

from nebulakit import NebulaContext
from nebulakit.core import constants as _constants
from nebulakit.core import hash as _hash_mixin
from nebulakit.core import hash as hash_mixin
from nebulakit.core.promise import create_and_link_node_from_remote
from nebulakit.exceptions import system as _system_exceptions
from nebulakit.exceptions import user as _user_exceptions
from nebulakit.loggers import remote_logger
from nebulakit.models import interface as _interface_models
from nebulakit.models import launch_plan as _launch_plan_model
from nebulakit.models import launch_plan as _launch_plan_models
from nebulakit.models import launch_plan as launch_plan_models
from nebulakit.models import task as _task_model
from nebulakit.models import task as _task_models
from nebulakit.models.admin.workflow import WorkflowSpec
from nebulakit.models.core import compiler as compiler_models
from nebulakit.models.core import identifier as _identifier_model
from nebulakit.models.core import identifier as id_models
from nebulakit.models.core import workflow as _workflow_model
from nebulakit.models.core import workflow as _workflow_models
from nebulakit.models.core.identifier import Identifier
from nebulakit.models.core.workflow import Node, WorkflowMetadata, WorkflowMetadataDefaults
from nebulakit.models.interface import TypedInterface
from nebulakit.models.literals import Binding
from nebulakit.models.task import TaskSpec
from nebulakit.remote import interface as _interface
from nebulakit.remote import interface as _interfaces
from nebulakit.remote.remote_callable import RemoteEntity


class NebulaTask(hash_mixin.HashOnReferenceMixin, RemoteEntity, TaskSpec):
    """A class encapsulating a remote Nebula task."""

    def __init__(
        self,
        id,
        type,
        metadata,
        interface,
        custom,
        container=None,
        task_type_version: int = 0,
        config=None,
        should_register: bool = False,
    ):
        super(NebulaTask, self).__init__(
            template=_task_model.TaskTemplate(
                id,
                type,
                metadata,
                interface,
                custom,
                container=container,
                task_type_version=task_type_version,
                config=config,
            )
        )
        self._should_register = should_register

    @property
    def id(self):
        """
        This is generated by the system and uniquely identifies the task.

        :rtype: nebulakit.models.core.identifier.Identifier
        """
        return self.template.id

    @property
    def type(self):
        """
        This is used to identify additional extensions for use by Propeller or SDK.

        :rtype: Text
        """
        return self.template.type

    @property
    def metadata(self):
        """
        This contains information needed at runtime to determine behavior such as whether or not outputs are
        discoverable, timeouts, and retries.

        :rtype: TaskMetadata
        """
        return self.template.metadata

    @property
    def interface(self):
        """
        The interface definition for this task.

        :rtype: nebulakit.models.interface.TypedInterface
        """
        return self.template.interface

    @property
    def custom(self):
        """
        Arbitrary dictionary containing metadata for custom plugins.

        :rtype: dict[Text, T]
        """
        return self.template.custom

    @property
    def task_type_version(self):
        return self.template.task_type_version

    @property
    def container(self):
        """
        If not None, the target of execution should be a container.

        :rtype: Container
        """
        return self.template.container

    @property
    def config(self):
        """
        Arbitrary dictionary containing metadata for parsing and handling custom plugins.

        :rtype: dict[Text, T]
        """
        return self.template.config

    @property
    def security_context(self):
        return self.template.security_context

    @property
    def k8s_pod(self):
        return self.template.k8s_pod

    @property
    def sql(self):
        return self.template.sql

    @property
    def should_register(self) -> bool:
        return self._should_register

    @property
    def name(self) -> str:
        return self.template.id.name

    @property
    def resource_type(self) -> _identifier_model.ResourceType:
        return _identifier_model.ResourceType.TASK

    @property
    def entity_type_text(self) -> str:
        return "Task"

    @classmethod
    def promote_from_model(cls, base_model: _task_model.TaskTemplate) -> NebulaTask:
        t = cls(
            id=base_model.id,
            type=base_model.type,
            metadata=base_model.metadata,
            interface=_interfaces.TypedInterface.promote_from_model(base_model.interface),
            custom=base_model.custom,
            container=base_model.container,
            task_type_version=base_model.task_type_version,
        )
        # Override the newly generated name if one exists in the base model
        if not base_model.id.is_empty:
            t._id = base_model.id

        return t


class NebulaTaskNode(_workflow_model.TaskNode):
    """A class encapsulating a task that a Nebula node needs to execute."""

    def __init__(self, nebula_task: NebulaTask):
        super(NebulaTaskNode, self).__init__(None)
        self._nebula_task = nebula_task

    @property
    def reference_id(self) -> id_models.Identifier:
        """A globally unique identifier for the task."""
        return self._nebula_task.id

    @property
    def nebula_task(self) -> NebulaTask:
        return self._nebula_task

    @classmethod
    def promote_from_model(cls, task: NebulaTask) -> NebulaTaskNode:
        """
        Takes the idl wrapper for a TaskNode,
        and returns the hydrated Nebulakit object for it by fetching it with the NebulaTask control plane.
        """
        return cls(nebula_task=task)


class NebulaWorkflowNode(_workflow_model.WorkflowNode):
    """A class encapsulating a workflow that a Nebula node needs to execute."""

    def __init__(
        self,
        nebula_workflow: NebulaWorkflow = None,
        nebula_launch_plan: NebulaLaunchPlan = None,
    ):
        if nebula_workflow and nebula_launch_plan:
            raise _system_exceptions.NebulaSystemException(
                "NebulaWorkflowNode cannot be called with both a workflow and a launchplan specified, please pick "
                f"one. workflow: {nebula_workflow} launchPlan: {nebula_launch_plan}",
            )

        self._nebula_workflow = nebula_workflow
        self._nebula_launch_plan = nebula_launch_plan
        super(NebulaWorkflowNode, self).__init__(
            launchplan_ref=self._nebula_launch_plan.id if self._nebula_launch_plan else None,
            sub_workflow_ref=self._nebula_workflow.id if self._nebula_workflow else None,
        )

    def __repr__(self) -> str:
        if self.nebula_workflow is not None:
            return f"NebulaWorkflowNode with workflow: {self.nebula_workflow}"
        return f"NebulaWorkflowNode with launch plan: {self.nebula_launch_plan}"

    @property
    def launchplan_ref(self) -> id_models.Identifier:
        """A globally unique identifier for the launch plan, which should map to Admin."""
        return self._nebula_launch_plan.id if self._nebula_launch_plan else None

    @property
    def sub_workflow_ref(self):
        return self._nebula_workflow.id if self._nebula_workflow else None

    @property
    def nebula_launch_plan(self) -> NebulaLaunchPlan:
        return self._nebula_launch_plan

    @property
    def nebula_workflow(self) -> NebulaWorkflow:
        return self._nebula_workflow

    @classmethod
    def _promote_workflow(
        cls,
        wf: _workflow_models.WorkflowTemplate,
        sub_workflows: Optional[Dict[Identifier, _workflow_models.WorkflowTemplate]] = None,
        tasks: Optional[Dict[Identifier, NebulaTask]] = None,
        node_launch_plans: Optional[Dict[Identifier, launch_plan_models.LaunchPlanSpec]] = None,
    ) -> NebulaWorkflow:
        return NebulaWorkflow.promote_from_model(
            wf,
            sub_workflows=sub_workflows,
            node_launch_plans=node_launch_plans,
            tasks=tasks,
        )

    @classmethod
    def promote_from_model(
        cls,
        base_model: _workflow_model.WorkflowNode,
        sub_workflows: Dict[id_models.Identifier, _workflow_model.WorkflowTemplate],
        node_launch_plans: Dict[id_models.Identifier, _launch_plan_model.LaunchPlanSpec],
        tasks: Dict[Identifier, NebulaTask],
        converted_sub_workflows: Dict[id_models.Identifier, NebulaWorkflow],
    ) -> Tuple[NebulaWorkflowNode, Dict[id_models.Identifier, NebulaWorkflow]]:
        if base_model.launchplan_ref is not None:
            return (
                cls(
                    nebula_launch_plan=NebulaLaunchPlan.promote_from_model(
                        base_model.launchplan_ref, node_launch_plans[base_model.launchplan_ref]
                    )
                ),
                converted_sub_workflows,
            )
        elif base_model.sub_workflow_ref is not None:
            # the workflow templates for sub-workflows should have been included in the original response
            if base_model.reference in sub_workflows:
                wf = None
                if base_model.reference not in converted_sub_workflows:
                    wf = cls._promote_workflow(
                        sub_workflows[base_model.reference],
                        sub_workflows=sub_workflows,
                        node_launch_plans=node_launch_plans,
                        tasks=tasks,
                    )
                    converted_sub_workflows[base_model.reference] = wf
                else:
                    wf = converted_sub_workflows[base_model.reference]
                return cls(nebula_workflow=wf), converted_sub_workflows
            raise _system_exceptions.NebulaSystemException(f"Subworkflow {base_model.reference} not found.")

        raise _system_exceptions.NebulaSystemException(
            "Bad workflow node model, neither subworkflow nor launchplan specified."
        )


class NebulaBranchNode(_workflow_model.BranchNode):
    def __init__(self, if_else: _workflow_model.IfElseBlock):
        super().__init__(if_else)

    @classmethod
    def promote_from_model(
        cls,
        base_model: _workflow_model.BranchNode,
        sub_workflows: Dict[id_models.Identifier, _workflow_model.WorkflowTemplate],
        node_launch_plans: Dict[id_models.Identifier, _launch_plan_model.LaunchPlanSpec],
        tasks: Dict[id_models.Identifier, NebulaTask],
        converted_sub_workflows: Dict[id_models.Identifier, NebulaWorkflow],
    ) -> Tuple[NebulaBranchNode, Dict[id_models.Identifier, NebulaWorkflow]]:
        block = base_model.if_else
        block.case._then_node, converted_sub_workflows = NebulaNode.promote_from_model(
            block.case.then_node,
            sub_workflows,
            node_launch_plans,
            tasks,
            converted_sub_workflows,
        )

        for o in block.other:
            o._then_node, converted_sub_workflows = NebulaNode.promote_from_model(
                o.then_node, sub_workflows, node_launch_plans, tasks, converted_sub_workflows
            )

        else_node = None
        if block.else_node:
            else_node, converted_sub_workflows = NebulaNode.promote_from_model(
                block.else_node, sub_workflows, node_launch_plans, tasks, converted_sub_workflows
            )

        new_if_else_block = _workflow_model.IfElseBlock(block.case, block.other, else_node, block.error)

        return cls(new_if_else_block), converted_sub_workflows


class NebulaGateNode(_workflow_model.GateNode):
    @classmethod
    def promote_from_model(cls, model: _workflow_model.GateNode):
        return cls(model.signal, model.sleep, model.approve)


class NebulaArrayNode(_workflow_model.ArrayNode):
    @classmethod
    def promote_from_model(cls, model: _workflow_model.ArrayNode):
        return cls(model._parallelism, model._node, model._min_success_ratio, model._min_successes)


class NebulaNode(_hash_mixin.HashOnReferenceMixin, _workflow_model.Node):
    """A class encapsulating a remote Nebula node."""

    def __init__(
        self,
        id,
        upstream_nodes,
        bindings,
        metadata,
        task_node: Optional[NebulaTaskNode] = None,
        workflow_node: Optional[NebulaWorkflowNode] = None,
        branch_node: Optional[NebulaBranchNode] = None,
        gate_node: Optional[NebulaGateNode] = None,
        array_node: Optional[NebulaArrayNode] = None,
    ):
        if not task_node and not workflow_node and not branch_node and not gate_node and not array_node:
            raise _user_exceptions.NebulaAssertion(
                "An Nebula node must have one of task|workflow|branch|gate|array entity specified at once"
            )
        # TODO: Revisit nebula_branch_node and nebula_gate_node, should they be another type like Condition instead
        #       of a node?
        self._nebula_task_node = task_node
        if task_node:
            self._nebula_entity = task_node.nebula_task
        elif workflow_node:
            self._nebula_entity = workflow_node.nebula_workflow or workflow_node.nebula_launch_plan
        else:
            self._nebula_entity = branch_node or gate_node or array_node

        super(NebulaNode, self).__init__(
            id=id,
            metadata=metadata,
            inputs=bindings,
            upstream_node_ids=[n.id for n in upstream_nodes],
            output_aliases=[],
            task_node=task_node,
            workflow_node=workflow_node,
            branch_node=branch_node,
            gate_node=gate_node,
            array_node=array_node,
        )
        self._upstream = upstream_nodes

    @property
    def task_node(self) -> Optional[NebulaTaskNode]:
        return self._nebula_task_node

    @property
    def nebula_entity(self) -> Union[NebulaTask, NebulaWorkflow, NebulaLaunchPlan, NebulaBranchNode]:
        return self._nebula_entity

    @classmethod
    def _promote_task_node(cls, t: NebulaTask) -> NebulaTaskNode:
        return NebulaTaskNode.promote_from_model(t)

    @classmethod
    def _promote_workflow_node(
        cls,
        wn: _workflow_model.WorkflowNode,
        sub_workflows: Dict[id_models.Identifier, _workflow_model.WorkflowTemplate],
        node_launch_plans: Dict[id_models.Identifier, _launch_plan_model.LaunchPlanSpec],
        tasks: Dict[Identifier, NebulaTask],
        converted_sub_workflows: Dict[id_models.Identifier, NebulaWorkflow],
    ) -> Tuple[NebulaWorkflowNode, Dict[id_models.Identifier, NebulaWorkflow]]:
        return NebulaWorkflowNode.promote_from_model(
            wn,
            sub_workflows,
            node_launch_plans,
            tasks,
            converted_sub_workflows,
        )

    @classmethod
    def promote_from_model(
        cls,
        model: _workflow_model.Node,
        sub_workflows: Optional[Dict[id_models.Identifier, _workflow_model.WorkflowTemplate]],
        node_launch_plans: Optional[Dict[id_models.Identifier, _launch_plan_model.LaunchPlanSpec]],
        tasks: Dict[id_models.Identifier, NebulaTask],
        converted_sub_workflows: Dict[id_models.Identifier, NebulaWorkflow],
    ) -> Tuple[Optional[NebulaNode], Dict[id_models.Identifier, NebulaWorkflow]]:
        node_model_id = model.id
        # TODO: Consider removing
        if id in {_constants.START_NODE_ID, _constants.END_NODE_ID}:
            remote_logger.warning(f"Should not call promote from model on a start node or end node {model}")
            return None, converted_sub_workflows

        nebula_task_node, nebula_workflow_node, nebula_branch_node, nebula_gate_node, nebula_array_node = (
            None,
            None,
            None,
            None,
            None,
        )
        if model.task_node is not None:
            if model.task_node.reference_id not in tasks:
                raise RuntimeError(
                    f"Remote Workflow closure does not have task with id {model.task_node.reference_id}."
                )
            nebula_task_node = cls._promote_task_node(tasks[model.task_node.reference_id])
        elif model.workflow_node is not None:
            nebula_workflow_node, converted_sub_workflows = cls._promote_workflow_node(
                model.workflow_node,
                sub_workflows,
                node_launch_plans,
                tasks,
                converted_sub_workflows,
            )
        elif model.branch_node is not None:
            nebula_branch_node, converted_sub_workflows = NebulaBranchNode.promote_from_model(
                model.branch_node,
                sub_workflows,
                node_launch_plans,
                tasks,
                converted_sub_workflows,
            )
        elif model.gate_node is not None:
            nebula_gate_node = NebulaGateNode.promote_from_model(model.gate_node)
        elif model.array_node is not None:
            nebula_array_node = NebulaArrayNode.promote_from_model(model.array_node)
            # TODO: validate task in tasks
        else:
            raise _system_exceptions.NebulaSystemException(
                f"Bad Node model, neither task nor workflow detected, node: {model}"
            )

        # When WorkflowTemplate models (containing node models) are returned by Admin, they've been compiled with a
        # start node. In order to make the promoted NebulaWorkflow look the same, we strip the start-node text back out.
        # TODO: Consider removing
        for model_input in model.inputs:
            if (
                model_input.binding.promise is not None
                and model_input.binding.promise.node_id == _constants.START_NODE_ID
            ):
                model_input.binding.promise._node_id = _constants.GLOBAL_INPUT_NODE_ID

        return (
            cls(
                id=node_model_id,
                upstream_nodes=[],  # set downstream, model doesn't contain this information
                bindings=model.inputs,
                metadata=model.metadata,
                task_node=nebula_task_node,
                workflow_node=nebula_workflow_node,
                branch_node=nebula_branch_node,
                gate_node=nebula_gate_node,
                array_node=nebula_array_node,
            ),
            converted_sub_workflows,
        )

    @property
    def upstream_nodes(self) -> List[NebulaNode]:
        return self._upstream

    @property
    def upstream_node_ids(self) -> List[str]:
        return list(sorted(n.id for n in self.upstream_nodes))

    def __repr__(self) -> str:
        return f"Node(ID: {self.id})"


class NebulaWorkflow(_hash_mixin.HashOnReferenceMixin, RemoteEntity, WorkflowSpec):
    """A class encapsulating a remote Nebula workflow."""

    def __init__(
        self,
        id: id_models.Identifier,
        nodes: List[NebulaNode],
        interface,
        output_bindings,
        metadata,
        metadata_defaults,
        subworkflows: Optional[List[NebulaWorkflow]] = None,
        tasks: Optional[List[NebulaTask]] = None,
        launch_plans: Optional[Dict[id_models.Identifier, launch_plan_models.LaunchPlanSpec]] = None,
        compiled_closure: Optional[compiler_models.CompiledWorkflowClosure] = None,
        should_register: bool = False,
    ):
        # TODO: Remove check
        for node in nodes:
            for upstream in node.upstream_nodes:
                if upstream.id is None:
                    raise _user_exceptions.NebulaAssertion(
                        "Some nodes contained in the workflow were not found in the workflow description.  Please "
                        "ensure all nodes are either assigned to attributes within the class or an element in a "
                        "list, dict, or tuple which is stored as an attribute in the class."
                    )

        self._nebula_sub_workflows = subworkflows
        template_subworkflows = []
        if subworkflows:
            template_subworkflows = [swf.template for swf in subworkflows]

        super(NebulaWorkflow, self).__init__(
            template=_workflow_models.WorkflowTemplate(
                id=id,
                metadata=metadata,
                metadata_defaults=metadata_defaults,
                interface=interface,
                nodes=nodes,
                outputs=output_bindings,
            ),
            sub_workflows=template_subworkflows,
        )
        self._nebula_nodes = nodes

        # Optional things that we save for ease of access when promoting from a model or CompiledWorkflowClosure
        self._tasks = tasks
        self._launch_plans = launch_plans
        self._compiled_closure = compiled_closure
        self._node_map = None
        self._name = id.name
        self._should_register = should_register

    @property
    def name(self) -> str:
        return self._name

    @property
    def nebula_tasks(self) -> Optional[List[NebulaTask]]:
        return self._tasks

    @property
    def should_register(self) -> bool:
        return self._should_register

    @property
    def nebula_sub_workflows(self) -> List[NebulaWorkflow]:
        return self._nebula_sub_workflows

    @property
    def entity_type_text(self) -> str:
        return "Workflow"

    @property
    def resource_type(self):
        return id_models.ResourceType.WORKFLOW

    @property
    def nebula_nodes(self) -> List[NebulaNode]:
        return self._nebula_nodes

    @property
    def id(self) -> Identifier:
        """
        This is an autogenerated id by the system. The id is globally unique across Nebula.
        """
        return self.template.id

    @property
    def metadata(self) -> WorkflowMetadata:
        """
        This contains information on how to run the workflow.
        """
        return self.template.metadata

    @property
    def metadata_defaults(self) -> WorkflowMetadataDefaults:
        """
        This contains information on how to run the workflow.
        :rtype: WorkflowMetadataDefaults
        """
        return self.template.metadata_defaults

    @property
    def interface(self) -> TypedInterface:
        """
        Defines a strongly typed interface for the Workflow (inputs, outputs). This can include some optional
        parameters.
        """
        return self.template.interface

    @property
    def nodes(self) -> List[Node]:
        """
        A list of nodes. In addition, "globals" is a special reserved node id that can be used to consume
        workflow inputs
        """
        return self.template.nodes

    @property
    def outputs(self) -> List[Binding]:
        """
        A list of output bindings that specify how to construct workflow outputs. Bindings can
        pull node outputs or specify literals. All workflow outputs specified in the interface field must be bound
        in order for the workflow to be validated. A workflow has an implicit dependency on all of its nodes
        to execute successfully in order to bind final outputs.
        """
        return self.template.outputs

    @property
    def failure_node(self) -> Node:
        """
        Node failure_node: A catch-all node. This node is executed whenever the execution engine determines the
        workflow has failed.
        """
        return self.template.failure_node

    @classmethod
    def get_non_system_nodes(cls, nodes: List[_workflow_models.Node]) -> List[_workflow_models.Node]:
        return [n for n in nodes if n.id not in {_constants.START_NODE_ID, _constants.END_NODE_ID}]

    @classmethod
    def _promote_node(
        cls,
        model: _workflow_model.Node,
        sub_workflows: Optional[Dict[id_models.Identifier, _workflow_model.WorkflowTemplate]],
        node_launch_plans: Optional[Dict[id_models.Identifier, _launch_plan_model.LaunchPlanSpec]],
        tasks: Dict[id_models.Identifier, NebulaTask],
        converted_sub_workflows: Dict[id_models.Identifier, NebulaWorkflow],
    ) -> Tuple[Optional[NebulaNode], Dict[id_models.Identifier, NebulaWorkflow]]:
        return NebulaNode.promote_from_model(model, sub_workflows, node_launch_plans, tasks, converted_sub_workflows)

    @classmethod
    def promote_from_model(
        cls,
        base_model: _workflow_models.WorkflowTemplate,
        sub_workflows: Optional[Dict[Identifier, _workflow_models.WorkflowTemplate]] = None,
        tasks: Optional[Dict[Identifier, NebulaTask]] = None,
        node_launch_plans: Optional[Dict[Identifier, launch_plan_models.LaunchPlanSpec]] = None,
    ) -> NebulaWorkflow:
        base_model_non_system_nodes = cls.get_non_system_nodes(base_model.nodes)

        node_map = {}
        converted_sub_workflows = {}
        for node in base_model_non_system_nodes:
            nebula_node, converted_sub_workflows = cls._promote_node(
                node, sub_workflows, node_launch_plans, tasks, converted_sub_workflows
            )
            node_map[node.id] = nebula_node

        # Set upstream nodes for each node
        for n in base_model_non_system_nodes:
            current = node_map[n.id]
            for upstream_id in n.upstream_node_ids:
                upstream_node = node_map[upstream_id]
                current._upstream.append(upstream_node)

        subworkflow_list = []
        if converted_sub_workflows:
            subworkflow_list = [v for _, v in converted_sub_workflows.items()]

        task_list = []
        if tasks:
            task_list = [t for _, t in tasks.items()]

        # No inputs/outputs specified, see the constructor for more information on the overrides.
        wf = cls(
            id=base_model.id,
            nodes=list(node_map.values()),
            metadata=base_model.metadata,
            metadata_defaults=base_model.metadata_defaults,
            interface=_interfaces.TypedInterface.promote_from_model(base_model.interface),
            output_bindings=base_model.outputs,
            subworkflows=subworkflow_list,
            tasks=task_list,
            launch_plans=node_launch_plans,
        )

        wf._node_map = node_map

        return wf

    @classmethod
    def _promote_task(cls, t: _task_models.TaskTemplate) -> NebulaTask:
        return NebulaTask.promote_from_model(t)

    @classmethod
    def promote_from_closure(
        cls,
        closure: compiler_models.CompiledWorkflowClosure,
        node_launch_plans: Optional[Dict[id_models, launch_plan_models.LaunchPlanSpec]] = None,
    ):
        """
        Extracts out the relevant portions of a NebulaWorkflow from a closure from the control plane.

        :param closure: This is the closure returned by Admin
        :param node_launch_plans: The reason this exists is because the compiled closure doesn't have launch plans.
            It only has subworkflows and tasks. Why this is unclear. If supplied, this map of launch plans will be
        """
        sub_workflows = {sw.template.id: sw.template for sw in closure.sub_workflows}
        tasks = {}
        if closure.tasks:
            tasks = {t.template.id: cls._promote_task(t.template) for t in closure.tasks}

        nebula_wf = cls.promote_from_model(
            base_model=closure.primary.template,
            sub_workflows=sub_workflows,
            node_launch_plans=node_launch_plans,
            tasks=tasks,
        )
        nebula_wf._compiled_closure = closure
        return nebula_wf


class NebulaLaunchPlan(hash_mixin.HashOnReferenceMixin, RemoteEntity, _launch_plan_models.LaunchPlanSpec):
    """A class encapsulating a remote Nebula launch plan."""

    def __init__(self, id, *args, **kwargs):
        super(NebulaLaunchPlan, self).__init__(*args, **kwargs)
        # Set all the attributes we expect this class to have
        self._id = id
        self._name = id.name

        # The interface is not set explicitly unless fetched in an engine context
        self._interface = None
        # If fetched when creating this object, can store it here.
        self._nebula_workflow = None

    @property
    def name(self) -> str:
        return self._name

    @property
    def nebula_workflow(self) -> Optional[NebulaWorkflow]:
        return self._nebula_workflow

    @classmethod
    def promote_from_model(cls, id: id_models.Identifier, model: _launch_plan_models.LaunchPlanSpec) -> NebulaLaunchPlan:
        lp = cls(
            id=id,
            workflow_id=model.workflow_id,
            default_inputs=_interface_models.ParameterMap(model.default_inputs.parameters),
            fixed_inputs=model.fixed_inputs,
            entity_metadata=model.entity_metadata,
            labels=model.labels,
            annotations=model.annotations,
            auth_role=model.auth_role,
            raw_output_data_config=model.raw_output_data_config,
            max_parallelism=model.max_parallelism,
            security_context=model.security_context,
        )
        return lp

    @property
    def id(self) -> id_models.Identifier:
        return self._id

    @property
    def is_scheduled(self) -> bool:
        if self.entity_metadata.schedule.cron_expression:
            return True
        elif self.entity_metadata.schedule.rate and self.entity_metadata.schedule.rate.value:
            return True
        elif self.entity_metadata.schedule.cron_schedule and self.entity_metadata.schedule.cron_schedule.schedule:
            return True
        else:
            return False

    @property
    def workflow_id(self) -> id_models.Identifier:
        return self._workflow_id

    @property
    def interface(self) -> Optional[_interface.TypedInterface]:
        """
        The interface is not technically part of the admin.LaunchPlanSpec in the IDL, however the workflow ID is, and
        from the workflow ID, fetch will fill in the interface. This is nice because then you can __call__ the=
        object and get a node.
        """
        return self._interface

    @property
    def resource_type(self) -> id_models.ResourceType:
        return id_models.ResourceType.LAUNCH_PLAN

    @property
    def entity_type_text(self) -> str:
        return "Launch Plan"

    def compile(self, ctx: NebulaContext, *args, **kwargs):
        fixed_input_lits = self.fixed_inputs.literals or {}
        default_input_params = self.default_inputs.parameters or {}
        return create_and_link_node_from_remote(
            ctx,
            entity=self,
            _inputs_not_allowed=set(fixed_input_lits.keys()),
            _ignorable_inputs=set(default_input_params.keys()),
            **kwargs,
        )  # noqa

    def __repr__(self) -> str:
        return f"NebulaLaunchPlan(ID: {self.id} Interface: {self.interface}) - Spec {super().__repr__()})"
